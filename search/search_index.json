{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TMSIS Data Documentation","text":"<p>This website provides documentation for our TMSIS datasets for researchers within / collaborating with the Yale Medicaid Lab. On the site, you can find resources explaining the structure of the datasets, recommendations for working with large datasets, and example code from other projects that have used the same data.</p>"},{"location":"#hive-partitioned-datasets","title":"Hive-Partitioned Datasets:","text":"<p>CMS gives us the data in a bespoke fixed-width file format called <code>.fts</code>. Working with these files is generally slow (as the file format is not optimized for database queries), and cumbersome (as one must implement their own parsing logic to read the files). We parse and format the TMSIS data into a series of hive partitioned parquet files, a standardized file format that is fast to read, supports modern database operations, and enforces strong types, so you never have to worry that your files have been read in incorrectly.</p> <p>To get started, with the files, you can log into Milgram and open the directory where they are stored on the server. The files are currently located at <code>/gpfs/milgram/pi/medicaid_lab/data/cms/ingested/TMSIS_TAF</code>.</p> <pre><code>cd /gpfs/milgram/pi/medicaid_lab/data/cms/ingested/TMSIS_TAF\n</code></pre> <p>Opening this directory, you will see the following sub-directories:</p> <pre><code>TMSIS_taf/\n\u251c\u2500\u2500 taf_demog_elig_base\n\u251c\u2500\u2500 taf_demog_elig_dates\n\u251c\u2500\u2500 taf_demog_elig_disability\n\u251c\u2500\u2500 taf_demog_elig_hh_spo\n\u251c\u2500\u2500 taf_demog_elig_mngd_care\n\u251c\u2500\u2500 taf_demog_elig_mny_flw_prsn\n\u251c\u2500\u2500 taf_demog_elig_waiver\n\u251c\u2500\u2500 taf_inpatient_header\n\u251c\u2500\u2500 taf_inpatient_line\n\u251c\u2500\u2500 taf_inpatient_occurrence\n\u251c\u2500\u2500 taf_long_term_header\n\u251c\u2500\u2500 taf_long_term_line\n\u251c\u2500\u2500 taf_long_term_occurrence\n\u251c\u2500\u2500 taf_other_services_header\n\u251c\u2500\u2500 taf_other_services_line\n\u251c\u2500\u2500 taf_other_services_occurrence\n\u251c\u2500\u2500 taf_rx_header\n\u2514\u2500\u2500 taf_rx_line\n</code></pre> <p>The directories fall into 5 groups, the pharmacy files (taf_rx_.*), the long-term care files (taf_long_term_.*), the inpatient files (taf_inpatient_.*), the other care files (taf_other_services_.*) and the demographic and eligibility files (taf_demog_elig_.*). You can find quick links to the ResDAC documentation for each file type at the bottom of this page.</p> <p>If we open the <code>taf_demog_elig_base</code> directory, we can see that the data is further partitioned at the state-year level. If you are performing data analysis on a state in a single year, you can simply take one of these files and start performing data analysis on it. Alternatively, if you are more familiar with <code>arrow</code> and <code>DuckDB</code>, you can use this file structure to speed up queries that are run on specific states or years. Check out the R examples for more information on how to do this if you are curious.</p> <pre><code>taf_demog_elig_base/\n\u251c\u2500\u2500 year=2016\n\u2502   \u251c\u2500\u2500 state=AK\n\u2502   \u2502   \u2514\u2500\u2500 data.paquet\n\u2502   \u251c\u2500\u2500 state=AL\n\u2502   \u2502   \u2514\u2500\u2500 data.paquet\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 year=2017\n\u2502   \u251c\u2500\u2500 state=AK\n\u2502   \u2502   \u2514\u2500\u2500 data.paquet\n\u2502   \u251c\u2500\u2500 state=AL\n\u2502   \u2502   \u2514\u2500\u2500 data.paquet\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"#resdac-documentation-links","title":"ResDAC Documentation Links:","text":"<p>ResDAC provides extensive documentation for all of the files we use. No column names or datatypes have been changed in formatting the dataset into <code>parquet</code> format, so all the documentation presented on this website applies to the standardized files as well as the raw files.</p> <ul> <li>Documentation for other services files</li> <li>Documentation for demographic and eligibility files</li> <li>Documentation for inpatient files</li> <li>Documentation for long term care files</li> <li>Documentation for pharmacy files</li> </ul>"},{"location":"reporting_an_issue/","title":"Reporting An Issue","text":"<p>If you find a data quality issue, issue with the documentation, or a bug in any of the pipeline code, please report it to us!  You can do so by either:</p> <ol> <li>Filing a github issue here for issues with documentation, or here for issues with the ingested data or pipeline.</li> <li>Emailing us at beniamino.green[at]tuta.com or anthony.lollo[at]yale.edu</li> </ol> <p>GitHub issues are preferred as they allow everyone to respond to and learn from the issue, but email works if you don't have a GitHub account.</p>"},{"location":"using_the_pipeline/","title":"Using The Data Ingestion Pipeline","text":"<p>We have constructed a code pipeline which takes data in the format provided by CMS and processes it into a set of standarized, high-performace <code>.parquet</code> files suitable for large-scale data analysis. This guide is intended for readers interested in learning more about the pipeline and those looking to importing new years of data.</p>"},{"location":"using_the_pipeline/#how-does-the-pipeline-work","title":"How Does the Pipeline Work?","text":"<p>At a very high level, the pipeline completes the following steps that are needed to transform the files from the format given by CMS to a format suitable for speedy analysis:</p> <ol> <li>Scan listed 'import' directories for files in the format given by CMS (<code>.fts</code> files with associated <code>.dat</code> headers). Create a listing of these files.</li> <li>Associate <code>.fts</code> files with their <code>.dta</code> header files, which provide the metadata necessary to parse them. Read the metadata files to create schemas for each file that is read in.</li> <li>Convert <code>.fts</code> files into <code>.parquet</code> files, which are faster and have a standardized schema that ensures data types are the same between files of the same type.</li> <li>Partition the <code>.parquet</code> files, which speeds up queries run on specific states / years.</li> </ol> <p>These steps are administered by a build system called Targets which keeps track of which steps have been run on each file provided by CMS, and will re-ingest files if they are changed, or if we make a change to the code responsible for processing them. Targets also watches for new data files, and ingests them when run. It automatically keeps track of files that have already been processed, and won't re-ingest an old file if nothing about it has changed. You can ask targets to run the pipeline in response to any changes using the <code>targets::tar_make()</code> command in R from inside the pipeline root directory. We also provide a <code>submit.sh</code> script that can be run using <code>sbatch</code> provisions the neccessary resources for the pipeline using SLURM, and sets it running.</p>"},{"location":"using_the_pipeline/#adding-new-datasets","title":"Adding New Datasets","text":"<p>We have tried to construct the pipeline with extenisibility in mind, and have taken steps to ensure that you can easily add new regions or types of data from CMS. To add new data, you should extract the data to a directory that is accessible by the pipeline and list this directory in the <code>input_directories</code> list in the <code>_targets.R</code> file. The relevant lines are highlighted below:</p> _targets.R<pre><code>library(targets)\nlibrary(arrow)\nlibrary(crew)\n\nsource('R/ingestion.R')\nsource('R/hive_partition.R')\n\ntar_option_set(\n  packages = c(\"arrow\", \"tidyverse\", \"stringr\", \"duckplyr\"),\n  controller = crew_controller_local(workers = 20,\n                     local_log_directory=\"job_outputs/\",\n                     local_log_join = TRUE,\n                     seconds_launch=90,\n                     seconds_interval=3)\n)\n\noptions(readr.num_threads=1)\nset_cpu_count(1)\n\ninput_directories &lt;-list( # (1)\n  \"/gpfs/milgram/pi/medicaid_lab/data/cms/raw/dua57871-ndumele/2018/\",\n  \"/gpfs/milgram/pi/medicaid_lab/data/cms/raw/dua57871-ndumele/2017/\",\n  \"/gpfs/milgram/pi/medicaid_lab/data/cms/raw/dua59119-busch/\"\n)\n</code></pre> <ol> <li>This list gives the locations the pipeline should look for new files to     ingest. To add a new data extact from TMSIS, unzip it into a directory,     then list that directory here.</li> </ol> <p>The source code for the ingestion pipeline can be found here.</p>"},{"location":"best_practices/python_best_practices/","title":"Best Practices for Python","text":""},{"location":"best_practices/python_best_practices/#using-venv-to-manage-dependencies","title":"Using <code>venv</code> to manage dependencies:","text":"<p>Dependency management on the HPC can be very difficult. The <code>venv</code> module allows you to set up a virtual environment that isolates the dependencies for each individual project by installing them into a project-specific folder. This is an intergral part of writing reproducible code and is useful for two reasons:</p> <ol> <li>Virtual environments allow you to clearly communicate your code's required dependencies to your collaborators</li> <li>Virtual environments ensures that updating dependencies for one project will never break dependencies for another project (this is an issue when two projects rely on different versions of the same package)</li> </ol> <p>To create a new virtual environment for your python, you can run the following command:</p> <pre><code>python -m venv {path_to_virtual_environment}\n</code></pre> <p>which creates a new virtual environment at the specified location. In the example repositories, the virtual environment is located in <code>./venv/</code>.</p> <p>To have the virtual environment manage your dependencies, you have to <code>activate</code> it, which is done with the following command:</p> <pre><code>source {path_to_virtual_environment}/bin/activate\n</code></pre> <p>While the virtual environment is activated, you can use pip as you normally would, but all changes you make by installing packages will be isolated to this project. When you are done using the virtual environment, you can either press <code>Ctrl+d</code> (if working interactively), or use the <code>deactivate</code> command which will return your shell to normal. To get a snapshot of the packages used in the venv (say, to generate a <code>requirements.txt</code> file), you can run <code>pip freeze</code>.</p> <p>You can check out more info about the <code>venv</code> module and virtual environments on the python language website</p>"},{"location":"best_practices/r_best_practices/","title":"Best Practices for R","text":""},{"location":"best_practices/r_best_practices/#using-renv-to-manage-dependencies","title":"Using <code>renv</code> to manage dependencies:","text":"<p>Dependency management on the HPC can be very difficult. <code>renv</code> allows you to set up a virtual environment in R that isolates the dependencies for each individual project. This is useful for two reasons:</p> <ol> <li>It allows you to clearly communicate your code's required dependencies to your collaborators</li> <li>Updating dependencies for one project will never break dependencies for another project</li> </ol> <p>Virtual environments have already been set up for all of the example projects, and the virtual environment will be activated whenever you open a new R session in these directories (so if you start your project by copying one of these directories, you are good to go). To set up <code>renv</code> on a new project, simply call <code>renv::init()</code> in R while in the project home folder. You can then use either <code>renv::install()</code> or <code>install.packages()</code> to install packages as normal.</p> <p>More info about getting started with <code>renv</code> can be found here.</p>"},{"location":"best_practices/r_best_practices/#using-duckplyr-to-conduct-large-scale-data-analysis","title":"Using <code>duckplyr</code> to conduct large-scale data analysis.","text":"<p>Another important skill is using a query engine like DuckDB to run queries on datasets without loading them into memory.</p> <p>If you don't work with large-scale data, you are probably most familiar with a workflow that looks something like this:</p> <ol> <li>Read the data into R using <code>read_csv</code> or an alternative</li> <li>Run models and analysis on the data using dplyr</li> <li>Save out the results to another file</li> </ol> <p>When datasets get large, this workflow begins to break down as it becomes very slow (if not impossible) to load the data into memory in order to analyze. This is where technologies like DuckDB can help - you can register data with DuckDB without reading it into memory, and then write queries that access and load only the relevant parts of your data for analysis, saving time and resources.</p> <p>Importantly, new packages like <code>duckplyr</code> provide you a way to do this while still using dplyr-syntax. The first example, <code>eligibility_example/</code> is a great example of this: using duckplyr allows us to compute summary statistics on a massive dataset, using dplyr-syntax,  without reading the entire dataset into memory.</p> <p>To learn more about <code>duckplyr</code> check out these resources:</p> <ul> <li>This post announcing duckplyr on duckdb.org</li> <li>This presentation by Kirill M\u00fcller at <code>posit::conf</code> introducing duckplyr and demonstrating some of its' features</li> <li>This talk by Hannes M\u00fchleisen at <code>posit::conf</code> talking about DuckDB, the query engine / database that backs up duckplyr</li> </ul>"},{"location":"examples/R_examples/example_1/","title":"Example 1 - Reproducing Eligibility Statistics","text":"<p>This example involves reproducing these eligibility statistics from DQATLAS, which give the number of enrollees in one of three age groups in 2019. Source code for this example can be found here.</p> <p>This task is a good point of introduction for users wanting to learn how to use Arrow and DuckDB to expeditiously run queries on large datasets, and for researchers wanting to learn the SLURM workflow.</p>"},{"location":"examples/R_examples/example_1/#step-1-writing-the-r-code","title":"Step 1: Writing the R Code","text":"<p>The first step is to write the R code needed to execute our query. We first register the eligibility dataset that we will be using with R using <code>duckplyr_df_from_parquet()</code>, which registers the datasets with R without reading them into memory.  This is important because the dataset would take a long time to load into R, and because we are only interested in a subset of the data (the age, state, year, and recipient ID columns).</p> <p>We then run some simple dplyr code to filter to the rows we are interested in (observations in 2019), and select only the appropriate columns. Importantly, while this code is written in expressive <code>dplyr</code> syntax, it's actually run using DuckDB, so it executes crazy-fast. The final steps summarize the number of observations in each age group, and transform the outcome into a layout closer to the one on the DQATLAS website.</p> example.R<pre><code>library(duckplyr)\nlibrary(tidyverse)\n\ndata_set &lt;- '/gpfs/milgram/pi/medicaid_lab/data/cms/ingested/unpartitioned_compressed/taf_demog_elig_base/*/*'\n\ndf &lt;- duckplyr_df_from_parquet(data_set) %&gt;% # Registers dataset with R\n  # Remove \"dummy\" records for members that only appear in claims data and never have an eligibility record\n  filter(MISG_ELGBLTY_DATA_IND == 0, year == 2019L) %&gt;%\n  select(MSIS_ID, AGE_GRP_CD, STATE_CD) %&gt;%\n  # Documentation uses AGE_GRP_CD, but note some people have AGE = -1 but AGE_GRP_CD = 1...\n  # 0-18 = AGE_GRP_CD in [1,2,3,4]\n  # 19-64 = AGE_GRP_CD in [5,6,7]\n  # 65+ = AGE_GRP_CD in [8,9,10]\n  mutate(age_group =\n           case_when(\n             AGE_GRP_CD %in% c(1,2,3,4) ~ \"0-18\",\n             AGE_GRP_CD %in% c(5,6,7) ~ \"19-64\",\n             AGE_GRP_CD %in% c(8,9,10) ~ \"65+\",\n             TRUE ~ \"Missing\"\n                     )\n           ) %&gt;%\n  summarize(\n    n = n_distinct(MSIS_ID),\n    .by = c(STATE_CD, age_group)\n            )\n\ndf &lt;- df %&gt;%\n  # \"Widen\" dataframe by making each age-group total a column\n  pivot_wider(id_cols = STATE_CD, names_from=age_group, values_from = n) %&gt;%\n  mutate(Missing = replace_na(Missing,0)) %&gt;%\n  mutate(N = `19-64` + `0-18` + `65+` + Missing) %&gt;%\n  arrange(STATE_CD)\n\nprint(df, n=100)\n</code></pre>"},{"location":"examples/R_examples/example_1/#step-2-writing-the-slurm-script","title":"Step 2: Writing the SLURM Script","text":"<p>To run analysis code on the cluster, you must submit a job to SLURM, the workload manager responsible for provisioning computer resources for users and running jobs that are submitted to it. This section shows how to write a SLURM job script to get the R code we have written running on the cluster.</p> <p>SLURM job scripts are essentially bash scripts with a special header which dictates the paramaters used to run the code. When the job is submitted, SLURM will wait for the necessary resources to become available and then run the bash script with the parameters you provide. A script we use to run our R code can be found below. To send this script to slurm, you can run <code>sbatch submit.sh</code> in the terminal.</p> submit.sh<pre><code>#!/usr/bin/bash\n#SBATCH --job-name=TMSIS_example_analysis\n#SBATCH --time=00:20:00\n#SBATCH --mail-type=all\n#SBATCH --partition=day\n#SBATCH --mem=40GB\n#SBATCH --cpus-per-task=5\n#SBATCH --output=out.txt\n#SBATCH -e err.txt\n\nmodule add R/4.3.0-foss-2020b # Load R\n\nRscript example.R # Run the R script\n</code></pre> <p>Parameters passed to slurm are fed in using lines that start with <code>#SBATCH --{parameter_name}</code>. The most important parameters are the following:</p> <ul> <li><code>--partition</code> which dictates which partition the job runs on. This is important to consider because different partitions have different resources available, so you may need to change your partition to run large jobs.</li> <li><code>--mem</code> which dictates the memory available to the analysis job. If you specify this too low, your job may run out of memory and crash. Setting this too high means that you may wait a long time for the requested resources to become available.</li> <li><code>--time</code> The maximum time allowed for the job to be run before it is terminated by SLURM. Again, there is a balancing act here between selecting enough time for your job to complete, without setting it so high that it is difficult for the job to be scheduled.</li> </ul> <p>You can read more about these parameters at either the SLURM website or the Yale HPC website, which has specific info about what options are available on Milgram.</p>"},{"location":"examples/python_examples/example_1/","title":"Example 1 - Reproducing Eligibility Statistics","text":"<p>This example involves reproducing these eligibility statistics from DQATLAS, which give the number of enrollees in one of three age groups in 2019. Source code for this example can be found here.</p> <p>This task is a good point of introduction for users wanting to learn how to use Arrow  and pyarrow and for researchers wanting to learn how to submit jobs on the HPC.</p>"},{"location":"examples/python_examples/example_1/#step-1-python-code","title":"Step 1: Python Code","text":"example.py<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport pyarrow.parquet as pq\nimport pyarrow.compute as pc\n\ndata_set = '/gpfs/milgram/pi/medicaid_lab/data/cms/ingested/dua57871-ndumele/2017/taf_demog_elig_base_res000019152_req011826/'\n\nstates = [\"AK\", \"AL\", \"AR\", \"AZ\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \"HI\",\n        \"IA\", \"ID\", \"IL\", \"IN\", \"KS\", \"KY\", \"LA\", \"MA\", \"MD\", \"ME\", \"MI\", \"MN\",\n        \"MO\", \"MS\", \"MT\", \"NC\", \"ND\", \"NE\", \"NH\", \"NJ\", \"NM\", \"NV\", \"NY\", \"OH\",\n        \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VA\", \"VT\", \"WA\",\n        \"WI\", \"WV\", \"WY\"]\n\nabbrev_to_name = { \"AK\": \"Alaska\", \"AL\": \"Alabama\", \"AR\": \"Arkansas\", \"AZ\":\n        \"Arizona\", \"CA\": \"California\", \"CO\": \"Colorado\", \"CT\": \"Connecticut\",\n        \"DE\": \"Delaware\", \"FL\": \"Florida\", \"GA\": \"Georgia\", \"HI\": \"Hawaii\",\n        \"IA\": \"Iowa\", \"ID\": \"Idaho\", \"IL\": \"Illinois\", \"IN\": \"Indiana\", \"KS\":\n        \"Kansas\", \"KY\": \"Kentucky\", \"LA\": \"Louisiana\", \"MA\": \"Massachusetts\",\n        \"MD\": \"Maryland\", \"ME\": \"Maine\", \"MI\": \"Michigan\", \"MN\": \"Minnesota\",\n        \"MO\": \"Missouri\", \"MS\": \"Mississippi\", \"MT\": \"Montana\", \"NC\": \"North Carolina\",\n        \"ND\": \"North Dakota\", \"NE\": \"Nebraska\", \"NH\": \"New Hampshire\",\n        \"NJ\": \"New Jersey\", \"NM\": \"New Mexico\", \"NV\": \"Nevada\", \"NY\": \"New York\",\n        \"OH\": \"Ohio\", \"OK\": \"Oklahoma\", \"OR\": \"Oregon\", \"PA\": \"Pennsylvania\",\n        \"RI\": \"Rhode Island\", \"SC\": \"South Carolina\", \"SD\": \"South Dakota\",\n        \"TN\": \"Tennessee\", \"TX\": \"Texas\", \"UT\": \"Utah\", \"VA\": \"Virginia\",\n        \"VT\": \"Vermont\", \"WA\": \"Washington\", \"WI\": \"Wisconsin\",\n        \"WV\": \"West Virginia\", \"WY\": \"Wyoming\"}\n\ncols = ['BENE_ID', 'STATE_CD', 'AGE', 'AGE_GRP_CD', 'MISG_ELGBLTY_DATA_IND', 'MSIS_ID']\ntable = pq.read_table(data_set, columns=cols)\nprint(table.nbytes/10**9)\n\nd = {}\nfor state in states:\n    df = table.filter(pc.equal(table['STATE_CD'], state)).to_pandas()\n    # Remove \"dummy\" records for members that only appear in claims data and never have an eligibility record\n    df = df[df['MISG_ELGBLTY_DATA_IND'].eq(0)]\n\n    # Documentation uses AGE_GRP_CD, but note some people have AGE = -1 but AGE_GRP_CD = 1...\n    # 0-18 = AGE_GRP_CD in [1,2,3,4]\n    # 19-64 = AGE_GRP_CD in [5,6,7]\n    # 65+ = AGE_GRP_CD in [8,9,10]\n    s = (pd.cut(pd.to_numeric(df['AGE_GRP_CD'], errors='coerce'), [0, 5, 8, np.inf], labels=['0-18', '19-64', '65+'], right=False)\n                       .value_counts(normalize=True).mul(100).round(1)\n                               )\n\n        # Seems reports use MSIS_ID, though BENE_ID is the better identifier for a member over time...\n    s['total'] = df.MSIS_ID.nunique()\n    s['missing'] = np.round(df.AGE_GRP_CD.isnull().mean()*100, 1)\n    d[abbrev_to_name.get(state)] = s\n\n# Join together all states.\nres = (pd.DataFrame.from_dict(d, orient='index').sort_index()\n                 .reindex(['total', 'missing', '0-18', '19-64', '65+'], axis=1)\n                       )\nres['total'] = res['total'].apply(lambda x: f'{x:,.0f}')\nres\n</code></pre>"},{"location":"examples/python_examples/example_1/#step-2-writing-the-slurm-script","title":"Step 2: Writing the SLURM Script","text":"<p>To run analysis code on the cluster, you must submit a job to SLURM, the workload manager responsible for provisioning computer resources for users and running jobs that are submitted to it. This section shows how to write a SLURM job script to get the R code we have written running on the cluster.</p> <p>SLURM job scripts are essentially bash scripts with a special header which dictates the paramaters used to run the code. When the job is submitted, SLURM will wait for the necessary resources to become available and then run the bash script with the parameters you provide. A script we use to run our R code can be found below. To send this script to slurm, you can run <code>sbatch submit.sh</code> in the terminal.</p> submit.sh<pre><code>#!/usr/bin/bash\n#SBATCH --job-name=TMSIS_python_example\n#SBATCH --time=00:20:00\n#SBATCH --mail-type=all\n#SBATCH --partition=day\n#SBATCH --mem=40GB\n#SBATCH --cpus-per-task=5\n#SBATCH --output=out.txt\n#SBATCH -e err.txt\n\n# Load Python\nmodule load Python/3.10.8-GCCcore-12.2.0\n\n# load the virtual environment used for this project\nsource venv/bin/activate\n\npython example.py\n</code></pre> <p>Parameters passed to slurm are fed in using lines that start with <code>#SBATCH --{parameter_name}</code>. The most important parameters are the following:</p> <ul> <li><code>--partition</code> which dictates which partition the job runs on. This is important to consider because different partitions have different resources available, so you may need to change your partition to run large jobs.</li> <li><code>--mem</code> which dictates the memory available to the analysis job. If you specify this too low, your job may run out of memory and crash. Setting this too high means that you may wait a long time for the requested resources to become available.</li> <li><code>--time</code> The maximum time allowed for the job to be run before it is terminated by SLURM. Again, there is a balancing act here between selecting enough time for your job to complete, without setting it so high that it is difficult for the job to be scheduled.</li> </ul> <p>You can read more about these parameters at either the SLURM website or the Yale HPC website, which has specific info about what options are available on Milgram.</p>"}]}